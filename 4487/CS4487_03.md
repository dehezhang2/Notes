## Lecture 09 Linear Dimensionality Reduction

### Dimensionality Reduction

* Definition of dimensionality reduction task:

  * Given a collection of feature vectors $\{\textbf x^{(i)} \in \R^n \}$, map the feature vectors into a lower dimensional space $\{\textbf z^{(i)} \in \R^l\}$, where $l<n​$, while preserving certain “properties” of the data

    ![Screen Shot 2019-10-31 at 11.41.32 AM](Screen Shot 2019-10-31 at 11.41.32 AM.png)

* Example

  * Representing Time Series

    ![Screen Shot 2019-10-31 at 11.43.26 AM](Screen Shot 2019-10-31 at 11.43.26 AM.png)

  * Representing Natural Image Patches

    ![Screen Shot 2019-10-31 at 11.44.55 AM](Screen Shot 2019-10-31 at 11.44.55 AM.png)

* Application

  * Can be used as a **pre-processing step** to enable more accurate classification and regression on manifold data
  * Very low dimensional embeddings (i.e., $l=2,3$) can be used to **visualize complex manifold-structured data** as in the previous example
  * Can be used to **“de-noise”** data by **projecting to lower-dimensional space** and then **projecting back to the feature space** 

* Dimensionality Reduction vs. Feature Selection

  * The goal of feature selection is to **remove features that are not informative** with respect to the class label. This obviously reduces the dimensionality of the feature space 
  * Dimensionality reduction can be used to find a **meaningful lower dimensional feature space** for manifold-structured data even when there is information in each of the feature dimensions so that none can be discarded (Linear independent)
  * Another important property of dimensionality reduction is that it is **unsupervised**. It will attempt to **preserve(保留) structure in the data that could be useful for a range of supervised problems** 
  * Unlike feature selection, which is a supervised task, dimensionality reduction can sometimes **discard structure useful to a particular supervised task** thereby increasing error 

* Dimensionality Reduction vs. Data Compression

  * While dimensionality reduction can be seen as a simplistic form of data compression, it is not equivalent to it, as the goal of data compression is to reduce the ***entropy*** of the representation **not only** the dimensionality
  * For example, in lossless data compression, *arithmetic coding* encodes the entire data into a single number, an arbitrary-precision fraction $q$ where $0.0 ≤ q < 1.0$. In some science fiction books, this is paraphrased as a pinpoint representing all information of the whole universe 

### Linear Dimensionality Reduction

* Linear Dimensionality Reduction: 

  * The simplest dimensionality reduction methods assumes that the observed high dimensional data vectors $\textbf x^{(i)} \in \R^n$ lie on an $l$-dimensional linear subspace with $\R^n$ (Project the data vectors to a lower dimensional space)

    ![1572670584804](1572670584804.png)

* Assumption: Firstly, assume all the data points are already on a lower dimensional space

  * Mathematically, the linear subspace assumption can be written as follows:
    $$
    \textbf x^{(i)} = \sum_{k=1}^l z^{(i)}_k \textbf b_k
    $$

  * Where $\textbf b_k = [b_{k_1},b_{k_2},...,b_{k_n}]^T$ for $b_{k_i}\ where\ i=1,...,l$ are a set of basis vectors describing an $l$-dimensional linear sub-space of $\R^n$ and $z^{(i)}_k$'s are real-valued weights on those basis vectors (reduced data vectors)

* Connection to linear regression

  * This expression is exactly linear regression where $x^{(i)}_j$ is the target, $z^{(i)}_k$‘’s are the weights, and $b_{kj}$ for each $k$ are the features
    $$
    x^{(i)}_j=\sum^l_{k=1}z^{(i)}_kb_{kj}
    $$

  * This observation is also true if we swap the roles of the weights and the features

  * However, unlike linear regression, we only know what corresponds to the targets. We must learn both the features and the weights

* Matrix Form

  * If we let $\textbf X$ be the data matrix $X_{ij}=x^{(i)}_j$, $\textbf Z$ be a matrix where $Z_{ik}=z^{(i)}_k$, and $\textbf B$ be a matrix where $B_{kj}=b_{kj}$, we can express $\textbf X$ as follows:
    $$
    \textbf X = \textbf Z \times \textbf B
    $$

  * $\textbf Z \in \R^{m\times l}$ is often referred to as the **factor loading matrix** while $\textbf B \in \R^{l\times n}$ are refered to as the **latent factors** 

  * Most real world data will be subject to noise. If we assume that $\epsilon \in \R^{m \times n}$ is 

### Principal Component Analysis

### Connection between PCA and SVD

