### Exhaustive Clustering

* Define a Clustering
  * Suppose we have $m$ data cases $D=\{\textbf x^{(i)}\}_{i=1,...,m}$ 
  * A clustering of the $m$ cases into $K$ clusters is a partitioning of $D$ into $K$ mutually disjoint subsets $C={C_1,...,C_k}$ such that $C_1\cup...\cup C_K=D$
  * Suppose we have a function $f(C)$ that takes a partitioning $C$ of the data set $D$ and returns a score with lower scores indicating better clustering
  * The optimal clustering according to $f$ is simply given by: $argmin_Cf(C)$
  * The complexity of exhaustive clustering depends on the total number of partitions
* Number of Clusterings
  * The total number of clusterings of a data set with $m$ elements is the Bell number $B_m$, where $B_0=B_1=1$ and $B_{m+1}=\sum^m_{k=0}{{m}\choose{k}}B_k$ 
  * Wrong: The recursive relation means that, for the first cluster we choose $m-k$ elements, and calculate how many ways of combination for $k$ elements left, and sum all the possibility of $k$‘s (It is wrong because for a partition, the order of choice doesn’t matter)
  * True proof: Let $x^{(1)}$ to be a fixed to the first cluster, we can choose $m-k$ elements in the same cluster with $x^{(1)}$ , then the order is fixed. Therefore, the equation is for $m+1$ instead of $m$
  * The complexity of exhaustive clustering scales with $B_m$ and is thus computationally totally intractable for general scoring functions
  * We will need either approximation algorithms or scoring functions with special properties

### K-Means Clustering

* The K-means algorithm is an iterative optimization algorithm for clustering that alternates between two steps

* The algorithm maintains a set of $K$ cluster centroids or prototypes $\{\textbf µ_k\}$ that represent the average (mean) feature vector of the data cases in each cluster 

* Algorithm: Suppose we let $z^{(i)}$ indicate which cluster $\textbf x^{(i)}$ belongs to and $\mu_k\in\R^n$ be the cluster centroid/prototype for cluster $k$. The two main steps of the algorithm can then be expressed as follows:

  * In the ﬁrst step, the distance between each data case and each prototype is computed, and each data case is assigned to the nearest prototype 

    ![1571754879633](1571754879633.png)

  * In the second step, the prototypes are updated to the mean of the data cases assigned to them

    ![1571754887382](1571754887382.png)

* Example:

  ![1571754941634](1571754941634.png)

* The K-Means Objective

  * The $K$-means algorithm attempts to minimize the sum of the within-cluster variation over all clusters (also called the within-cluster sum of squares):

    ![1571755101731](1571755101731.png)

  * It can be shown that $K$-means is exactly coordinate descent on $l$. Speciﬁcally, the assignment step minimizes $l$ with respect to $z$ while holding $µ$ ﬁxed, and the update step minimizes $l$ with respect to µ while holding $z$ ﬁxed. Thus, must monotonically decrease, and the value of must converge

  * Note that $l$ has many local optimal in general, each corresponding to a different clustering of the data. Finding the global optimum is not computationally tractable => **highly sensitive to initialization**

* Initialization

  * It is common to perform multiple random re-starts of the algorithm and take the clustering with the minimal total variation
  * Common initializations include setting the initial centers to be randomly selected data cases, setting the initial partition to a random partition, and selecting centers using a “furthest ﬁrst”-style heuristic (more formally known as K-means++)
  * It often helps to initially to run with $K log(K)$ clusters, then merge clusters to get down to $K$ and run the algorithm from that initialization

* Issues

  * Only works with Euclidean distance. An alternative based on Manhattan distance is called the K-medians algorithm 
  * Pre-processing like re-scaling/normalizing features can completely change the results 
  * We need some way to determine the “right” number of clusters to focus on. We want to cluster on salient differences between data cases, not noise 
  * The number of iterations to convergence is often small (like 20), but examples can be constructed that require an exponential number of steps to converge 
  * Results in a hard assignment of data cases to clusters, which may be a problem if there are outliers

### Expectation Maximization



### Gaussian Mixture Models

* Mixture Models

  * A mixture model is a probabilistic clustering model that is the unsupervised analogue(模拟，近似) of the Bayes optimal classiﬁer where the **unknown assignment of data cases** to clusters take the place of the known class labels
  * We let $\textbf x^{(i)}$ be a data case and $z^{(i)} \in \{1,…,K\}$ be the index of the cluster $\textbf x^{(i)}$ belongs to. $z^{(i)}$ is often called the mixture indicator variable or the latent class (i.e. ubobserved)
  * Each cluster $k​$ specifies it’s ==own distribution over the feature vectors==$p(\textbf x^{(i)}|z^{(i)}=k)​$
  * We also have a discrete distribution $p(z^{(i)}=k)=\theta_k​$, which describes the prior probability that $\textbf x^{(i)}​$ belongs to cluster $k​$ 

* Data Distribution

  * The joint distribution of the feature($\textbf x^{(i)}$) and the mixture indicator variable($z^{(i)}$) is :
    $$
    p(\textbf x^{(i)}, z^{(i)}=k) = p(\textbf x^{(i)}|z^{(i)}=k)p(z^{(i)}=k)
    $$

  * In clustering, ==we don’t know that the right value of the mixture indicator variable is a priori,== but we can marginalize it away to obtain a probability distribution on the feature vector only
    $$
    p(\textbf x^{(i)}) = \sum^K_{k=1}p(\textbf x^{(i)}|z^{(i)}=k)p(z^{(i)}=k)
    $$

* Mixture Component Distributions: Common model choices for $p(\textbf x|z=k)$ are:

  * Categorical: $\Pi^n_{j=1}\Pi_{x\in X_j}\theta^{\mathbb I[x_j=x]}_{x,j|k}$ , where $X_j$ contains all possibilities of $j_{th}$ feature of $\textbf x$ 
  * Independent Gaussian: $\Pi^n_{j=1}N(x_j;\mu_{j|k}, \sigma^2_{j|k})$ 
  * Multivariate Gaussian: $N(\textbf x, \textbf μ_k, \textbf Σ_k)$ 

* Learning:

  * Given data set $D=\{\textbf x^{(i)}\}_{i=1,…,m}$, we can learn the mixture model parameters by maximizing the log probability of the data give the parameters:
    $$
    l=\sum^m_{i=1}log(\sum^K_{k=1}p(\textbf x^{(i)}|z^{(i)}=k)p(z^{(i)}=k))
    $$

  * While we can do this directly using gradient-based optimization,
    it’s often faster to use a special algorithm called *Expectation
    Maximization*

* Expectation Maximization for Gaussian Mixture Models

  * E-Step: In the first step of the algorithm, we compute the posterior probability that each data case belongs to each cluster using the Bayes rule:
    $$
    w^{(i)}_k = p(z^{(i)}=k|\textbf x^{(i)}) = {\theta_kN(\textbf x^{(i)};\textbf μ_k, \textbf Σ_k) \over \sum^K_{k'=1}\theta_{k'}N(\textbf x^{(i)};\textbf μ_{k'}, \textbf Σ_{k'})}
    $$

  * M-Step: In the second step, we update the parameters using the weighted averages:

    ![](Screen Shot 2019-10-25 at 1.09.36 PM.png)

* A Special Case

  * Suppose we ==fix $\theta_k={1 \over K}$ and $\sum_k = I$==. In this case we have:
    $$
    N(\textbf x; \textbf μ_k,\textbf Σ_k)={1 \over \sqrt{|(2\pi)^nI|}}exp(-{1\over 2}||\textbf x^{(i)}-\textbf μ_k||^2_2)
    $$
    

  * and we obtain the following special case of the EM algorithm for multivariate Gaussians:

    ![](Screen Shot 2019-10-25 at 1.25.20 PM.png)

    

* Trade-Offs

  * We can see that the original *K*-means algorithm performs **hard assignments during clustering**, and implicitly assumes all clusters will have an equal number of points assigned as well as a unit covariance matrix 
  * EM for Gaussian mixture models relaxes all of these assumptions. The objective still has multiple local optima, but **EM also produces a guaranteed non-decreasing sequence of objective function values** 
  * EM can also be used with any component densities/distributions to customize the model to a given data set 
  * As with *K*-means, initialization is important, but the same heuristics can be applied 
