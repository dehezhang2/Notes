## Lecture 09 Linear Dimensionality Reduction

### Dimensionality Reduction

* Definition of dimensionality reduction task:

  * Given a collection of feature vectors $\{\textbf x^{(i)} \in \R^n \}$, map the feature vectors into a lower dimensional space $\{\textbf z^{(i)} \in \R^l\}$, where $l<n$, while preserving certain “properties” of the data

    ![Screen Shot 2019-10-31 at 11.41.32 AM](Screen Shot 2019-10-31 at 11.41.32 AM.png)

* Example

  * Representing Time Series

    ![Screen Shot 2019-10-31 at 11.43.26 AM](Screen Shot 2019-10-31 at 11.43.26 AM.png)

  * Representing Natural Image Patches

    ![Screen Shot 2019-10-31 at 11.44.55 AM](Screen Shot 2019-10-31 at 11.44.55 AM.png)

* Application

  * Can be used as a **pre-processing step** to enable more accurate classification and regression on manifold data
  * Very low dimensional embeddings (i.e., $l=2,3$) can be used to **visualize complex manifold-structured data** as in the previous example
  * Can be used to **“de-noise”** data by **projecting to lower-dimensional space** and then **projecting back to the feature space** 

* Dimensionality Reduction vs. Feature Selection

* Dimensionality Reduction vs. Data Compression

### Linear Dimensionality Reduction

### Principal Component Analysis

### Connection between PCA and SVD

