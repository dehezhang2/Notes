## Lecture 09 Linear Dimensionality Reduction

### Dimensionality Reduction

* Definition of dimensionality reduction task:

  * Given a collection of feature vectors $\{\textbf x^{(i)} \in \R^n \}$, map the feature vectors into a lower dimensional space $\{\textbf z^{(i)} \in \R^l\}$, where $l<n​$, while preserving certain “properties” of the data

    ![Screen Shot 2019-10-31 at 11.41.32 AM](Screen Shot 2019-10-31 at 11.41.32 AM.png)

* Example

  * Representing Time Series

    ![Screen Shot 2019-10-31 at 11.43.26 AM](Screen Shot 2019-10-31 at 11.43.26 AM.png)

  * Representing Natural Image Patches

    ![Screen Shot 2019-10-31 at 11.44.55 AM](Screen Shot 2019-10-31 at 11.44.55 AM.png)

* Application

  * Can be used as a **pre-processing step** to enable more accurate classification and regression on manifold data
  * Very low dimensional embeddings (i.e., $l=2,3$) can be used to **visualize complex manifold-structured data** as in the previous example
  * Can be used to **“de-noise”** data by **projecting to lower-dimensional space** and then **projecting back to the feature space** 

* Dimensionality Reduction vs. Feature Selection

  * The goal of feature selection is to **remove features that are not informative** with respect to the class label. This obviously reduces the dimensionality of the feature space 
  * Dimensionality reduction can be used to find a **meaningful lower dimensional feature space** for manifold-structured data even when there is information in each of the feature dimensions so that none can be discarded (Linear independent)
  * Another important property of dimensionality reduction is that it is **unsupervised**. It will attempt to **preserve structure in the data that could be useful for a range of supervised problems** 
  * Unlike feature selection, which is a supervised task, dimensionality reduction can sometimes **discard structure useful to a particular supervised task** thereby increasing error 

* Dimensionality Reduction vs. Data Compression

  * While dimensionality reduction can be seen as a simplistic form of data compression, it is not equivalent to it, as the goal of data compression is to reduce the *entropy* of the representation **not only** the dimensionality
  * For example, in lossless data compression, *arithmetic coding* encodes the entire data into a single number, an arbitrary-precision fraction $q$ where $0.0 ≤ q < 1.0​$. In some science fiction books, this is paraphrased as a pinpoint representing all information of the whole universe 

### Linear Dimensionality Reduction

* Linear Dimensionality Reduction: 
  * The simplest dimensionality reduction methods assumes that the observed high dimensional data vectors $\textbf x^{(i)} \in \R^n$ lie on an $l$-dimensional
* 

### Principal Component Analysis

### Connection between PCA and SVD

