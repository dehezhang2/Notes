### Exhaustive Clustering

* Define a Clustering
  * Suppose we have $m$ data cases $D=\{\textbf x^{(i)}\}_{i=1,...,m}$ 
  * A clustering of the $m$ cases into $K$ clusters is a partitioning of $D$ into $K$ mutually disjoint subsets $C={C_1,...,C_k}$ such that $C_1\cup...\cup C_K=D$
  * Suppose we have a function $f(C)$ that takes a partitioning $C$ of the data set $D$ and returns a score with lower scores indicating better clustering
  * The optimal clustering according to $f$ is simply given by: $argmin_Cf(C)$
  * The complexity of exhaustive clustering depends on the total number of partitions
* Number of Clusterings
  * The total number of clusterings of a data set with $m$ elements is the Bell number $B_m$, where $B_0=B_1=1$ and $B_{m+1}=\sum^m_{k=0}{{m}\choose{k}}B_k$ 
  * Wrong: The recursive relation means that, for the first cluster we choose $m-k$ elements, and calculate how many ways of combination for $k$ elements left, and sum all the possibility of $k$‘s (It is wrong because for a partition, the order of choice doesn’t matter)
  * True proof: Let $x^{(1)}$ to be a fixed to the first cluster, we can choose $m-k$ elements in the same cluster with $x^{(1)}$ , then the order is fixed. Therefore, the equation is for $m+1$ instead of $m$
  * The complexity of exhaustive clustering scales with $B_m$ and is thus computationally totally intractable for general scoring functions
  * We will need either approximation algorithms or scoring functions with special properties

### K-Means Clustering

* The K-means algorithm is an iterative optimization algorithm for clustering that alternates between two steps

* The algorithm maintains a set of $K$ cluster centroids or prototypes $\{\textbf µ_k\}$ that represent the average (mean) feature vector of the data cases in each cluster 

* Algorithm: Suppose we let $z^{(i)}$ indicate which cluster $\textbf x^{(i)}$ belongs to and $\mu_k\in\R^n$ be the cluster centroid/prototype for cluster $k$. The two main steps of the algorithm can then be expressed as follows:

  * In the ﬁrst step, the distance between each data case and each prototype is computed, and each data case is assigned to the nearest prototype 

    ![1571754879633](1571754879633.png)

  * In the second step, the prototypes are updated to the mean of the data cases assigned to them

    ![1571754887382](1571754887382.png)

* Example:

  ![1571754941634](1571754941634.png)

* The K-Means Objective

  * The $K$-means algorithm attempts to minimize the sum of the within-cluster variation over all clusters (also called the within-cluster sum of squares):

    ![1571755101731](1571755101731.png)

  * It can be shown that $K$-means is exactly coordinate descent on $l$. Speciﬁcally, the assignment step minimizes $l$ with respect to $z$ while holding $µ$ ﬁxed, and the update step minimizes $l$ with respect to µ while holding $z$ ﬁxed. Thus, must monotonically decrease, and the value of must converge

  * Note that $l$ has many local optimal in general, each corresponding to a different clustering of the data. Finding the global optimum is not computationally tractable => **highly sensitive to initialization**

* Initialization

  * It is common to perform multiple random re-starts of the algorithm and take the clustering with the minimal total variation
  * Common initializations include setting the initial centers to be randomly selected data cases, setting the initial partition to a random partition, and selecting centers using a “furthest ﬁrst”-style heuristic (more formally known as K-means++)
  * It often helps to initially to run with $K log(K)$ clusters, then merge clusters to get down to $K$ and run the algorithm from that initialization

* Issues

  * Only works with Euclidean distance. An alternative based on Manhattan distance is called the K-medians algorithm 
  * Pre-processing like re-scaling/normalizing features can completely change the results 
  * We need some way to determine the “right” number of clusters to focus on. We want to cluster on salient differences between data cases, not noise 
  * The number of iterations to convergence is often small (like 20), but examples can be constructed that require an exponential number of steps to converge 
  * Results in a hard assignment of data cases to clusters, which may be a problem if there are outliers

### Gaussian Mixture Models

* Mixture Models
  * A mixture model is a probabilistic clustering model that is the unsupervised analogue of the Bayes optimal classiﬁer where the unknown assignment of data cases to clusters take the place of the known class labels

