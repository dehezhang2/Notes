## Lecture 12: Neural Networks and Deep Learning

### Overview

* Feature Learning

  ![1575024492656](1575024492656.png)

* Grand Challenge: Why/how deep learning works

  * Gap between engineering (or art) and science: Lack of theoretical understandings guarantees, and analytical tools
  * Training is computationally expensive and difficult, relying on many “magics”
    * Structure design
    * Network initialization
    * Hyperparameter tuning
  * No principled way to incorporate(包括) domain expertise(专长)
  * No principled way to interpret the model behaviors

### Neural Networks

* Mcculloch and Pitts Neuron (1943)

  ![1575025060253](1575025060253.png)

* The Perceptron (感受器) (for classification problems)
  * The Perceptron is a simple online algorithm for adapting the weights in a McCulloch/Pitts neuron. It was developed in the1950s by Rosenblatt at Cornell
    * Start with all-zero weight vector $\textbf w^{(0)}$, and initialize $t$ to $0$ 
    * For all $ (\textbf x^{(i)}, y^{(i)}) \in D$, compute the activation $\textbf a^{(i)} = (x^{(i)})^T\textbf w^{(t)}$
    * If $\textbf y^{(i)} \textbf a^{(i)} < 0$, then $\textbf w^{(t+1)} = \textbf w^{(t)}+y^{(i)}\textbf x^{(i)}$ and $t\leftarrow t+ 1$ 
  * If $y^{(i)} = 1$ and $\hat {y}^{(i)} = -1$, the activation is initially negative and will be increased
  * If $y^{(i)} = -1$ and $\hat {y}^{(i)} = 1$, the activation is initially positive and will be decreased
* The Perceptron Theorem
  * Let the training set be $\textbf D = \{(\textbf x^{(i)}, y^{(i)}),i=1,...,m\}$. Assume that $||\textbf x^{(i)}||_2 \leq \alpha$ for all $i$ and further that there exists a unit-length vector $\textbf u$ ($||\textbf u||_2=1$) such that $y^{(i)}(\textbf u^T\textbf x^{(i)}) \geq \beta$ for all examples in $D$. Then the total number of mistakes that the perceptron algorithm makes on this sequence is at most $(\alpha / \beta)^2$ 