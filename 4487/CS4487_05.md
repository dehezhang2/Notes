## Lecture 12: Neural Networks and Deep Learning

### Overview

* Feature Learning

  ![1575024492656](1575024492656.png)

* Grand Challenge: Why/how deep learning works

  * Gap between engineering (or art) and science: Lack of theoretical understandings guarantees, and analytical tools
  * Training is computationally expensive and difficult, relying on many “magics”
    * Structure design
    * Network initialization
    * Hyper-parameter tuning
  * No principled way to incorporate(包括) domain expertise(专长)
  * No principled way to interpret the model behaviors

### Neural Networks

* Mcculloch and Pitts Neuron (1943)

  ![1575025060253](1575025060253.png)

* The Perceptron (感受器) (for **binary** classification problems)
  * The Perceptron is a simple online algorithm for adapting the weights in a McCulloch/Pitts neuron. It was developed in the1950s by Rosenblatt at Cornell
    * Start with all-zero weight vector $\textbf w^{(0)}$, and initialize $t$ to $0$ 
    
    * For all $ (\textbf x^{(i)}, y^{(i)}) \in D$, compute the activation $\textbf a^{(i)} = (x^{(i)})^T\textbf w^{(t)}$
    
    * If $\textbf y^{(i)} \textbf a^{(i)} < 0$, then $\textbf w^{(t+1)} = \textbf w^{(t)}+y^{(i)}\textbf x^{(i)}$ and $t\leftarrow t+ 1$ 
      $$
      \textbf a^{(i)}_{t+1} = (x^{(i)})^Tw^{(t+1)} \\
      =(x^{(i)})^T(w^{(t)}+y^{(i)}\textbf x^{(i)}) \\
      =a^{(i)}_t + y^{(i)}(x^{(i)})^Tx^{(i)}
      $$
      
    
    
    
    ![image-20191229173524476](image-20191229173524476.png)
    
  * If $y^{(i)} = 1$ and $\hat {y}^{(i)} = -1$, the activation ($\hat{y}^{(i)}$) is initially negative and will be increased (see the equation)
  
  * If $y^{(i)} = -1$ and $\hat {y}^{(i)} = 1$, the activation is initially positive and will be decreased
  
* The Perceptron Theorem
  
  * Let the training set be $\textbf D = \{(\textbf x^{(i)}, y^{(i)}),i=1,...,m\}$. Assume that $||\textbf x^{(i)}||_2 \leq \alpha$ for all $i$ and further that there exists a unit-length vector $\textbf u$ ($||\textbf u||_2=1$) such that $y^{(i)}(\textbf u^T\textbf x^{(i)}) \geq \beta$ (weighted average of dimensions $\textbf x^{(i)}$ times $y^{(i)}$) for all examples in $D$. Then the total number of mistakes that the perceptron algorithm makes on this sequence is at most $(\alpha / \beta)^2$ 
  
  * Proof:
  
    * The perceptron updates its weights only for following case:
      $$
      (x^{(i)})^Tw^{(t)}y^{(i)}\leq 0
      $$
      
  
    * According the rule of perceptron learning
      $$
      u^Tw^{(t+1)} = u^T(w^{(t)}+y^{(i)}x^{(i)}) \geq u^Tw^{(t)}+\beta
      $$
      
  
    * The weight matrix is initialized as $0$ :
      $$
      u^Tw^{(t+1)} \geq u^Tw^{(1)} + t\beta \geq t \beta
      $$
  
    * Therefore:
      $$
      ||w^{(t+1)}||^2_2 = ||w^{(t)} +y^{(i)}x^{(i)} ||^2_2 \\
      =||w^{(t)}||^2_2 + ||y^{(i)}x^{(i)}||^2_2 + 2y^{(i)}(x^{(i)})^Tw^{(t)} \\
      =||w^{(t)}||^2_2 + ||x^{(i)}||^2_2 + 2y^{(i)}(x^{(i)})^Tw^{(t)} \\
      \leq ||w^{(t)}||^2_2 + ||x^{(i)}||^2_2 \\
      \leq ||w^{(t)}||^2_2 + \alpha^2 \leq ||w^{(1)}||^2_2 + t\alpha^2 = t\alpha^2
      $$
      
  
    * Therefore:
      $$
      \sqrt{t}\alpha = ||w^{(t+1)}||_2 \\
      = ||w^{(t+1)}||_2 ||u||_2 \geq ||w^{(t+1)}||_2 ||u||_2 cos\ \theta \\
      = u^T w^{(t+1)} \geq t\beta
      $$
  
    * Which implies that: 
      $$
      t \leq ({\alpha \over \beta})^2
      $$
  
    * Hence, if the perceptron made  a $t$-th mistake, $t$ must be less than or equal to $({\alpha \over \beta})^2$. That is, the perceptron always converges as long as the data set $D$ is linearly separable (i.e. there exists a unit-length vector $u$ s.t. $y^{(i)}(\textbf u^T\textbf x^{(i)}) \geq \beta$ for $\forall\ x^{(i)} \in D$)
  
  * Limitations of Single Layer Perceptrons
  
    * The representational power of the single-layer perceptron was not well understood at first in the AI community
    * In 1969, Minsky and Papert at MIT popularized a set of arguments showing that the single-layer perceptron could not learn certain classes of functions (including XOR)
    * They also showed that more complex functions could be represented using a multi-layer perceptron or MLP, but no one knew how to learn them from examples
    * This led to a shift away from mathematical/statistical models in AI toward logical/symbolic models
  
* Multi-Layer Perceptron

  ![image-20191229182456578](image-20191229182456578.png)

  * The solution to MLP learning turned out to be: 

    * Make the hidden layer non-linearities smooth (sigmoid/logistic) functions: 
      $$
      h^{(1)}_j = {1 \over {1 + exp(-(\sum_kw^{(1)}_{jk}x_k + b^{(1)}_j ))}} \\
      h^{(t)}_j = {1 \over {1 + exp(-(\sum_kw^{(t)}_{jk}h_j^{(t-1)} + b^{(t)}_j ))}} \\
      $$
      

    * Make the output layer non-linearity a smooth function

    * Use standard numerical optimization methods (gradient descent) to learn the parameters. The algorithm is known as back propagation and was popularized by Rumelhart, Hinton and Willianms in the 1980s

  * Activation Functions

    ![image-20191229183102487](image-20191229183102487.png)

  * 

