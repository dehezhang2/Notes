# CS4487 Machine Learning

## Lecture 1: Introduction

* Course Information
  * Content
    * Lecture: Learning algorithm & Mathmetical Proof
    * Tutorial: Use learning algorithms on small examples
    * HW: four => derive and analyze machine learning algorithms with mathematical tools
  * Grade: 
    * Coursework(40)
      * 10(tutorial)
      * 30(assignment)
    * midterm(30)
    * final(30)
  * Book: 
    * Pattern Recognition & Machine Learning (C M B)
    * Convex Optimization (S B & L V)
    * The Matrix Cookbook (K B P)
    * Linear Algebra and Its Applications
    * Deep Learning
  * Syllabus:
    * Classification, regression, clustering, dimensionality reduction and deep learning
    * Model selection, model evaluation, regularization, design of experiments
  * Prerequisites:
    * Linear Algebra
    * Calculus
    * Probability and Statistics
    * Optimization
    * Python
  * Some problem
    * derivative of matrix with respect to another
    * maximum likelihood estimation(MLE) & maximum a posteriori estimation(MAP), and their relationship 
    * difference between gradient descent and stochastic gradient descent
    * sigular value decomposition(SVD) and principal component analysis (PCA) and their relationship 
* Introduction
  * type
    * By I/O
      * Supervised: Learning to predict (with a set of input and output)
      * Unsupervised: Learning to organize and represent
    * By Parameter
      * Non-parametric: storing the training data
      * Parametric: Using an algorithm to adapt the parameters in a mathematical or statistical model given training data
  * Sample => feature => training=>learned model
  * AI > ML (Data) > DL (multi-layer)
* Math
  * Vector Space (column vector)
  * Matrix
    * Square, diagonal, identity
    * Transpose, trace, inverse, **determinant**
    * **eigenvalue, eigenvectors**, orthogonality
  * Probability Concepts
    * Probability Distribution
    * Random Variable (X): Maps each element(get 2 balls) of the sample space to a value x(2) in X(called the range of the random variable)
    * Marginalization
    * Conditioning
    * Bayes Rules
    * Expectations
    * Classical Distributions (Bernoulli, Multinomial, Gaussian)

------------

## Lecture 2: KNN and Naive Bayes

### Classification

* Definition: 

  ![](Screen Shot 2019-09-05 at 5.28.56 PM.png)

* Classifier Learning

  ![](Screen Shot 2019-09-05 at 5.30.06 PM.png)

* Error and Accuracy

  ![](Screen Shot 2019-09-05 at 5.30.54 PM.png)

### KNN

* Definition

  ![](Screen Shot 2019-09-05 at 5.32.00 PM.png)

  * Notice that $argmax$ will traverse all the element in a set

  * Work with any distance function $d$ satisefying non-negativity$d(x,x’)>0$ and identity of indiscernibles $d(x,x)=0$

  * Alternatively, KNN can work with any similarity function $s$s atisfying non-negativity $s(x, x') ≥ 0$ that attains it’s maximum on indiscernibles $s(x, x) = max_{x'} s(x, x')$

  * Distance Metrics

    ![](Screen Shot 2019-09-05 at 5.39.31 PM.png)

  * Variant

    * If there are 4 very far point and 1 very close point => vote to very far

    * For each class $y$, traverse all the $k$ closest points, add all weights that belongs to the class and divide by some of weight of all $k$ closest points, where the weight of $i_{th}$ point is defined as $exp(-\alpha d_i)$. Then, choose the class with the maximum value

      ![](Screen Shot 2019-09-05 at 5.43.38 PM.png)

  * [**K-d tree** to reduce the complexity](<https://zhuanlan.zhihu.com/p/23966698>): Make use of the division line

    * Point to search: (-1, -5)

    ![](Screen Shot 2019-09-08 at 11.49.51 AM.png)

    ![](Screen Shot 2019-09-06 at 2.23.49 PM.png)

  * Trade off

    * Low bias: Converges to the correct decision surface as data goes to infinity
    * High variance: Lots of variability in the decision surface when amount of data is low
    * Curse of dimensionality: Everything is far from everything else in high dimensions
    * Space and time complexity: Need to store all training data. Take time at run time to compute

### Bayes Optimal Classifiers

* Probabilistic Classification

  * Ture probability of seeing a data case that belongs to $p(Y=y)=\phi_y​$

  * Ture probability density of seeing a data vector $x \in \R^n​$ that belongs to class $y​$ is $p(X=x|Y=y)=\varphi_y(x)​$

  * To calculate with vector $x​$ the probability of any one of $y​$ : 
    $$
    p(Y=y|X=x)={{p(X=x|Y=y)p(Y=y)}\over{\sum_{\tilde{y}\in Y}p(X=x|Y=\tilde{y})p(Y=\tilde{y})}}={{\varphi_y(x)\phi_y}\over{\sum_{\tilde{y}\in Y}}\varphi_\tilde{y}(x)\phi_\tilde{y}}
    $$

* Bayes Optimal Classifiers
  $$
  f_B(x)=arg max_{y\in Y}\ p(Y=y|X=x)=arg max_{y\in Y}\ \varphi_y(x)\phi_y
  $$
  
  $$
  MIN\ Error=1-E_x[max_{y\in Y}p(Y=y|X=x)]
  $$
  

  * $p(X=x)​$ can be treated as a constant for specific input vector

  * $E_x$ represents expectation with the condition $X=x​$ 

  * Proof:

    ![IMG_E2195024B9BE-1](IMG_E2195024B9BE-1.jpeg)

    ![IMG_9E22B345A5C5-1](IMG_9E22B345A5C5-1.jpeg)

    ![IMG_3877A3D9C846-1](IMG_3877A3D9C846-1.jpeg)

  * Not useful in practice: difficult to estimate $\varphi_y(x)$ in high dimensions

-----------

## Lecture03: Naive Bayes, Linear Discriminant Analysis and Logistic Regression

### Naive Bayes

* Naive bayes classifier approximates the Bayes optimal classifier using a simple form for the functions $\varphi_y(x)$ 

* Assumes that all of the data dimensions are statistically independent given the value of the class variable
  $$
  \varphi_y(x)=p(X=x|Y=y)=\Pi^n_{j=1}p(X_j=x_j|Y=y)=\Pi^n_{j=1}\varphi_{j|y}(x_j)
  $$

* The general form for the classification function:
  $$
  f_{NB}(x)=argmax_{y\in Y}\ \phi_y\ \Pi^n_{j=1}\varphi_{j|y}(x_j)
  $$

* The functions $\varphi_{j|y}(x_j)=p(X_j=x_j|Y=y)$ are called *marginal class conditional distributions* 

  * For real valued $x_j$, $p(X_j=x_j|Y=y)$ is typically modeled as a Gaussian density $\varphi_{j|y}(x_j)=N(x_j;\mu_{j|y}, \sigma_{j|y}^2)$ 
  * For binary valued $x_j$ ,$p(X_j=x_j|Y=y)$ is typically modeled as a Bernoulli distribution $\varphi_{j|y}(x_j)=\theta_{j|y}^{x_j}(1-\theta_{j|y})^{(1-x_j)}$  
  * For general categorical values $x_j$, $p(X_j=x_j|Y=y)$ is typically modeled as a categorical distribution $\varphi_{j|y}(x_j)=\Pi_{x\in X_j}\theta_{x,j|y}^{I[x_j=x]}$   

* The problem to maximize is how to choose a correct $\phi_y​$ and $\varphi_{j|y}(x_j)​$ => using the *maximum likelihood* over $D=\{(x^{(i)},y^{(i)}), i=1,…,m\}​$ 

  * **posterior probability**: $P(Y=y|X=x)$,  … :  likelihood $P(X=x|Y=y)$ , Prior probability: $P(Y=y) ​$ 
  * maximum likelihood: 让取样获得的大前提($P(D)$)可能性最大, $D$ 为数据集

  ![](Screen Shot 2019-09-12 at 7.51.02 PM.png)

  * for Bernoulli distribution, $x_j​$ is $0​$ or $1​$
  * proof of the class probabilities $\phi_y$ and the categorical probability $\theta_{x,j|y}$ => see lecture note

* Geometric Interpretation: for normal distribution of $x$, and binary classification problem we have =>

  ![](Screen Shot 2019-09-20 at 11.35.43 AM.png)

  ![](Screen Shot 2019-09-20 at 11.36.02 AM.png)

* Laplace Smoothing

  * The maximum likelihood : 

    ![](Screen Shot 2019-09-20 at 11.52.28 AM.png)

  * If the training set is small, it’s possible that $I[x_j^{(i)}=x]=0, for\ any\ i\in[1,m],i\ is\ integer$  

  * Cause the probability to be zero, which is overfitting

  * Laplace smoothing method:

    ![](Screen Shot 2019-09-20 at 11.56.54 AM.png)

  * When $\alpha=1$ it’s called Laplace smoothing

  * it can be shown that Laplace smoothing is the posterior mean with Dirichlet distribution as the conjugate prior for Categorical distribution

* Trade offs

  * Speed: learning and classification are fast
  * Storage: $O(n)$ parameters, large compression of the training data
  * Interpretability: good because $\varphi_y(x)$ is the class conditional averages
  * Accuracy: feature independence assumption and canocial forms for class-conditional marginals will rarely be correct for real-world problems, leading to lower accuracy 
  * Data: Some care is needed in the estimation of parameters in the
    discrete case when data is scarce(small data set)

### Linear Discriminant Analysis

* Linear Discriminant Analysis

  * A different approximation to the Bayes optimal classifier for real-valued data (instead of Maximum Likelihood)

  * Use dependent Normal Distribution in Naive Bayes, LDA assumes $\varphi_y(x)=N(x; \mu_y, \sum)$ , where $\sum​$ is covariance matrix

    ![](Screen Shot 2019-09-20 at 5.16.24 PM.png)

    ![](Screen Shot 2019-09-20 at 12.41.26 PM.png)

  * By using maximum likelihood, which reduces to using sample estimates:

    ![](Screen Shot 2019-09-20 at 5.28.18 PM.png)

  * similar to the one dimensional case, the decision boundary consists of points $x$ where:

    ![](Screen Shot 2019-09-20 at 5.37.23 PM.png)

* Trade-offs

  * Speed: quadratic dependence on $n​$ dimension makes LDA slower than Naive Bayes
  * Storage: $O(n^2)$ parameters, good when $n<<m​$ 
  * Interpretability: Good interpretability
  * Accuracy: Rarely be correct in real-world problems. However, the induced linear decision boundaries can often perform reasonably well
  * Data: LDA will generally need more data than NB since it needs to estimate the *O*(*n*2) parameters in the pooled covariance matrix

### Logistic Regression

* Generative vs. Discriminative Classifiers

  * Naive Bayes and LDA are said to be *generative* classifier because they explicityly model the joint distribution $p(x,y)$ of the data vectors $x$ and the lablel $y$ 
  * To build a probabilistic classifier, all we really need to model is $p(y|x)$ 
  * classifiers based on directly estimating $p(y|x)​$ are called *discriminative* classifiers because they ignore the distribution of $x​$ and focus only on the class labels $y​$ 

* Logistic Regression

  * In binary case, it directly models the decision boundary using a linear function:

    ![](Screen Shot 2019-09-20 at 6.25.52 PM.png)

  * Now the classifunction is : $f_{LR}=argmax_{y\in Y}p(Y=y|x)$: 

    * the standard logistic function, or the sigmoid function is defined as:
      $$
      g(z)={1\over{1+e^{-z}}} \\
      g'(z)=-({1\over{1+e^{-z}}})^2*(-e^{-z})\\
      =g(z)(1-g(z))
      $$

* The learning logistic regression

  * logistic regression model parameters $\theta={\{w,b\}}​$ are selected to optimize the conditional log likelihood of the labels given the data set $D=\{(x^{(i)},y^{(i)}),i=1,…,m\}​$ : 

    ![](Screen Shot 2019-09-20 at 7.06.01 PM.png)

  * cannot be maximized analytically. Learning the model parameters requires numerical optimization methods

* Multiclass Logistic Regression

  * Logistic regression extends to multiclass case:

    ![](Screen Shot 2019-09-20 at 7.15.16 PM.png)

  * [why different from binary case?](<https://blog.csdn.net/huangjx36/article/details/78056375>)

* Geometry

  * **Explicityly designed** to have a **linear decision boundary in the binary case**
  * In the multiclass case, the decision boundary is piece-wise linear
  * Same representational capacity as LDA

* Trade-offs

  * Speed: Faster than NB and LDA in classification time, numberical optimization is slower than naive bayes
  * Storage: O(n) parameters
  * Interpretability: The “importance” of different feature variables
    $x^j$ can be understood in terms of their weights $w^j$
  * Accuracy: Tends to be better in high dimensions with limited
    data compared to LDA. Much worse than KNN in low dimensions with **lots of data and non-linear decision boundaries** 

* Probability view of logistic function(sigmoid function)

  ![](Screen Shot 2019-09-20 at 8.44.51 PM.png)

  

-----------

## Lecture 04: Gradient Descent and Stochastic Gradient Descent

### Gradient Descent

* Gradient Descent

  * typical form of machine learning problem

    ![](Screen Shot 2019-09-20 at 8.40.09 PM.png)

  * for logistic regression, trying to minimize the negative log likelihood:

    ![](Screen Shot 2019-09-20 at 8.42.10 PM.png)

  * choose initial point $\theta^{(0)} \in \R^n$ and repeats

    ![](Screen Shot 2019-09-20 at 9.05.05 PM.png)

    ![](Screen Shot 2019-09-20 at 9.05.43 PM.png)

  * why gradient descent works

    ![IMG_0188](IMG_0188.jpg)

    ![](Screen Shot 2019-09-20 at 9.26.24 PM.png)

    ![](Screen Shot 2019-09-20 at 9.26.45 PM.png)

    ![](Screen Shot 2019-09-20 at 9.28.09 PM.png)

    ![](Screen Shot 2019-09-20 at 9.28.13 PM.png)

* Stochastic Gradient Descent

  * Compute loss for all the training data to get average takes $O(m)$ times

  * Can only choose one training data(i.e. one vecvtor) to (point)estimate the  true gradient

    ![](Screen Shot 2019-09-20 at 9.30.34 PM.png)

    ![](Screen Shot 2019-09-20 at 9.30.48 PM.png)

    ![](Screen Shot 2019-09-20 at 9.32.08 PM.png)

![](Screen Shot 2019-09-20 at 9.31.15 PM.png)

* Mini-Batch Gradient Descent

  * use a subset of samples => interval estimate => mean value theorem

    ![](Screen Shot 2019-09-20 at 9.33.40 PM.png)

* Gradient Descent of Logistic Regression

  ![](Screen Shot 2019-09-20 at 9.41.59 PM.png)

--------------

## Lecture05: Linear Regression

### Regression

* Regression: Given a feature vector $x \in {\R}^n$, predict it’s corresponding output value $y \in \R$ 
* Questions:
  * Does size of sample matters? => we prefere larger sample size
  * Does location of points matters? => Samples need be representative or informative
  * Given a set of $m$ points, is curve fitting unique? => Estimating “continuous” model from “discrete” data is all ill-posed problem and can never be “certain”
  * How to choose the best model from infinite curves? => Have some criteria choose prefered model 

* Example: House price, weather prediction, age estimation ...

* Regression learning problem:

  ![](Screen Shot 2019-09-22 at 2.33.06 PM.png)

* Error measure:

  * MSE:

    ![](Screen Shot 2019-09-22 at 2.34.09 PM.png)

    * SSE and RSS are the same

  * MAE:

    ![](Screen Shot 2019-09-22 at 2.34.30 PM.png)

### Linear Regression

* A parametric regression method that assumes the relationship betweeen $y$ and $x$ is a linear function with parameters $w=[w_1,w_2,…,w_n]^T$ and $b​$ 

  ![](Screen Shot 2019-09-22 at 2.37.07 PM.png)

* Ordinary Least Squares (OLS) Linear Regression

  * Minimize the MSE on the training data set:

    ![](Screen Shot 2019-09-22 at 2.38.43 PM.png)

  * Use derivative:

    ![IMG_FDE66967E6D0-1](IMG_FDE66967E6D0-1.jpeg)

* General OLS Solution

  * Assume that $X \in \R^{m\times (n+1)}$ is a data matrix with one data case $x^{(i)} \in \R^{n+1}$ per row, where $x^{(i)}_0=1$ and $y \in \R^m$ is a column vector containing the corresponding outputs. $w \in \R^{n+1}$ where $w_0=b$ 

  * Use derivative:

    ![IMG_0189](IMG_0189.jpg)

    ![IMG_0190](IMG_0190.jpg)

  * Use projection:

    ![IMG_4B18B13BB819-1](IMG_4B18B13BB819-1.jpeg)

* Connection to Probabilistic Models

  ![](Screen Shot 2019-09-22 at 3.41.49 PM.png)

  ![](Screen Shot 2019-09-22 at 3.42.34 PM.png)

  * Proof: 

    ![](Screen Shot 2019-10-02 at 10.27.20 AM.png)

* Normal Equations

  ![](Screen Shot 2019-09-22 at 3.46.45 PM.png)

  * Pseudo-inverse

    ![](Screen Shot 2019-09-22 at 3.47.18 PM.png)

  * For invertable $X$ pseudo-inverse is just inverse

  * When $X^TX$ is non-invertable:

    * $m<n+1$ => increase sample set and regularization
    * $m>n+1$ => singular  (remove redundant features)
      * Reason: $rank(X^TX)=rank(X)$ 

  * $n$ to large => GD

  * $m$ to large => SGD / mini-batch GD

* Strengths and Limitations of OLS
  * Need at least $m \geq n+1$ data cases to learn a model with an $n$ dimensional feature vector. Otherwise **inverse of** $X^TX$ is not defined
  * Very sensitive to noise and outliers due to MSE objective function/Normally distributed residuals assumption (prefer many medium error than one large error)
  * Sensitive to co-linear features ($x_i \approx ax_j+b$) . Otherwise inverse of $X^TX$ is not numerically stable
  * Computation is cubic in data dimension $n$ 

---------

## Lecture06: Regularized Linear Regression: Ridge, and Lasso & Overfitting, Regularization, and Cross Validation

### Regularized Linear Regression

* Reason of regularization

  * Control the parameter space to avoid *overfitting*

  * Obtain numberically more stable solutions

  * Enforce the desired parameter space as a prior

  * What is the price that we pay? The regularization term will bias the least squares estimate (not exactly least square)

  * Probabilistix interpretation:

    ![IMG_ED884420F04B-1](IMG_ED884420F04B-1.jpeg)

* General framework:

  ![](Screen Shot 2019-10-02 at 10.39.37 AM.png)

  * For ridge regression, $q=2$. Closed-form solution exists
  * For lasso regression, $q=1$. It is a quadratic programming problem with no closed-form solution in general. However, it can be solved efficiently using an algorithm called *least angle regression*. The advantage is that lasso simultaneously performs regularization and **encourages sparsity** (as a way of feature selection)
  * When $q<1$, the problem is non-convex, but **encourages more sparsity**

* The relationship between the **weight matrix** of regularized version and linear regression version

  * assume that: $X^TX=I​$ (each feature vector are unit orthogonal basis)

  * Linear regression: $min_w{1\over 2}||Xw-y||^2_2$ 
    $$
    w_{LS}=(X^TX)^{-1}X^Ty=X^Ty
    $$

  * Ridge regression: $min_w{1\over 2}||Xw-y||^2_2+{\lambda\over2}||w||^2_2$  

    ​	
    $$
    w_{RR}=(X^TX+\lambda I)^{-1}X^Ty=X^Ty/(1+\lambda)=W_{LS}/(1+\lambda)
    $$
    

  * Lasso regression: $min_w{1\over 2}||Xw-y||^2_2+{\lambda}||w||_1$ 

    
    $$
    w_{RR}=(X^TX+\lambda I)^{-1}X^Ty=X^Ty/(1+\lambda)=W_{LS}/(1+\lambda)
    $$

  * Proof:

    ![IMG_0191](IMG_0191.jpg)

    ![IMG_0192](IMG_0192.jpg)

  * Geometrical meaning (with x-axis $W_{LR}​$)

    ![](Screen Shot 2019-10-02 at 12.04.38 PM.png)

* Lasso vs Ridge (2D case)

  * Property of the minimum point

  ![IMG_0193](IMG_0193.jpg)

  

  * Graph of Lasso regression and Ridge regression:

    ![](Screen Shot 2019-10-02 at 2.13.04 PM.png)

    * it’s easier to have the intersection on the corner for $l_1$ case
    * one of $w_j$ will  be zero, which means the feature is dropped => sparsity

* Strengths and Limitations of Ridge and Lasso

  * Need at least $m \geq n+1$ data cases to learn a model with an $n+1$ dimensional feature vector (similar as linear regression)
  * Work when $X^TX$ is close to singular => select feature
  * MSE objective function is still sensitive to noice points
  * Do not solve **bias problem**
  * Computation for ridge is still cubic in data dimension $n$ with additional cost to determine regularization parameters. Computation for lasso is similar

